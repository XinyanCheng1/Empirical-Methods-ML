{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# _Multiple_ Linear Regression (MLR) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the population we have <ins>scalar</ins> random variables, $[Y,X_1,\\ldots,X_{k},e]$, that fulfill the following relationship\n",
    "\n",
    "$$\n",
    "Y=\\beta_0+\\beta_1 X_1+\\ldots+\\beta_k X_k+e\\text{,}\n",
    "$$\n",
    "\n",
    "where $E[e|X_1,\\ldots,X_{k}]=0$, there are no exact linear relationships among the set of regressors (covariates, confounders, independent variables, predictors, etc.) $[X_1,\\ldots,X_{k}]$, and var$(e|X_1,\\ldots,X_{k})<+\\infty$. The scalar random variable, $Y$, is called the outcome, or the dependent variable.\n",
    "\n",
    "\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Conditional_expectation\" style=\"color: #cc0000\">Conditional Expectation</a></p>\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Conditional_variance\" style=\"color: #cc0000\">Conditional Variance</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We observe a _random sample_ of $n$ observations taken from $[Y,X_1,\\ldots,X_{k},e]$, i.e., $\\{(y_i,x_{i,1},\\ldots,x_{i,k}):i=1,\\ldots,n\\}$. Therefore one has\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "y_{1}=\\beta_{0}+\\beta_{1}x_{1,1}+\\cdots+\\beta_{k}x_{1,k}+e_{1}\\\\\n",
    "y_{2}=\\beta_{0}+\\beta_{1}x_{2,1}+\\cdots+\\beta_{k}x_{2,k}+e_{2}\\\\\n",
    "y_{3}=\\beta_{0}+\\beta_{1}x_{3,1}+\\cdots+\\beta_{k}x_{3,k}+e_{3}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}=\\beta_{0}+\\beta_{1}x_{n,1}+\\cdots+\\beta_{k}x_{n,k}+e_{n}\n",
    "\\end{array}\n",
    "$$ or equivalently\n",
    "$$\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "y_{3}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}\n",
    "\\end{array}\n",
    "\\right]  =\\left[\n",
    "\\begin{array}\n",
    "[c]{cccc}\n",
    "1 & x_{1,1} & \\cdots & x_{1,k}\\\\\n",
    "1 & x_{2,1} & \\cdots & x_{2,k}\\\\\n",
    "1 & x_{3,1} & \\cdots & x_{3,k}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{n,1} & \\cdots & x_{n,k}\n",
    "\\end{array}\n",
    "\\right]  \\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "\\beta_{0}\\\\\n",
    "\\beta_{1}\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_{k}\n",
    "\\end{array}\n",
    "\\right]  +\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "e_{1}\\\\\n",
    "e_{2}\\\\\n",
    "e_{3}\\\\\n",
    "\\vdots\\\\\n",
    "e_{n}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "that can be rewritten in **matrix form**  as\n",
    "\n",
    "$$\n",
    "\\mathbf{y}=\\mathbf{X}\\mathbf{\\beta}+\\mathbf{e}\\text{,}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{y}$ is a $n\\times 1$ [vector](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)), $\\mathbf{X}$ is a $n\\times(k+1)$ [matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics)) - sometimes called the [*design matrix*](https://en.wikipedia.org/wiki/Design_matrix), $\\mathbf{\\beta}$ is a $(k+1)\\times 1$ vector of <ins>unknown</ins> parameters, and $\\mathbf{e}$ is a $n\\times 1$ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## removing everything from memory\n",
    "%R rm(list=ls())\n",
    "## turning all warnings off\n",
    "%R options(warn=-1)\n",
    "\n",
    "## installing the 'wooldridge' package if not previously installed\n",
    "%R if (!require(wooldridge)) install.packages('wooldridge')\n",
    "\n",
    "## loading the packages\n",
    "%R library(wooldridge)\n",
    "\n",
    "%R data(hprice2)\n",
    "\n",
    "##  hprice2\n",
    "##  Obs:   506\n",
    "\n",
    "##  1. price                    median housing price, $\n",
    "##  2. crime                    crimes committed per capita\n",
    "##  3. nox                      nitrous oxide, parts per 100 mill.\n",
    "##  4. rooms                    avg number of rooms per house\n",
    "##  5. dist                     weighted dist. to 5 employ centers\n",
    "##  6. radial                   accessibiliy index to radial hghwys\n",
    "##  7. proptax                  property tax per $1000\n",
    "##  8. stratio                  average student-teacher ratio\n",
    "##  9. lowstat                  % of people 'lower status'\n",
    "## 10. lprice                   log(price)\n",
    "## 11. lnox                     log(nox)\n",
    "## 12. lproptax                 log(proptax)\n",
    "\n",
    "%R head(hprice2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now transfer the `hprice2` R data frame into a pandas data frame and call it `py_hprice2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "  py_hprice2 = ro.conversion.rpy2py(ro.r('hprice2'))\n",
    "py_hprice2.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can access the `hprice2` pandas data frame directly from the `wooldridge` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wooldridge as woo\n",
    "hprice2 = woo.dataWoo('hprice2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**<span style=\"color:blue\">Assumption MLR.1:</span>** $\\mathbf{y}=\\mathbf{X}\\mathbf{\\beta}+\\mathbf{e}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ins>Example</ins>: We specify the following model \n",
    "\n",
    "$$\n",
    "\\texttt{lprice}=\\beta_{0}+\\beta_{1}\\texttt{lnox}+\\beta_{2}\\texttt{lproptax}+\\beta_{3}\\texttt{crime}+\\beta_{4}\\texttt{rooms}+\\beta_{5}\\texttt{dist}+\\beta_{6}\\texttt{radial}+\\beta_{7}\\texttt{stratio}+\\beta_{8}\\texttt{lowstat}+e\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "💻 We can use ```as.formula``` to specify the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## specifying the outcome variable (y) and regressors (X)\n",
    "%R outcome <- 'lprice'\n",
    "%R predictors <- c('lnox', 'lproptax', 'crime', 'rooms', 'dist', 'radial', 'stratio', 'lowstat')\n",
    "\n",
    "## creating a specification of the linear model\n",
    "%R f <- as.formula(paste(outcome,paste(predictors, collapse = ' + '),sep = ' ~ '))\n",
    "%R print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'lprice'\n",
    "predictors = [\"lnox\", \"lproptax\", \"crime\", \"rooms\", \"dist\", \"radial\", \"stratio\", \"lowstat\"]\n",
    "f = outcome + \"~\" + \"+\".join(predictors)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<span style=\"color:blue\">Assumption MLR.2:</span>** $\\{(y_i,x_{i,1},\\ldots,x_{i,k}):i=1,\\ldots,n\\}$ is a random sample (independent and identically distributed - i.i.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "💻 ```head``` allows us to visualize the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# R\n",
    "%R head(subset(hprice2,select=c(outcome,predictors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python\n",
    "hprice2[[outcome] + predictors].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<span style=\"color:blue\">Assumption MLR.3:</span>** rank$[E(\\mathbf{x}_i\\mathbf{x}_i^\\prime)]=k+1$, where $\\mathbf{x}_i^\\prime=[1,x_{i,1},\\ldots,x_{i,k}]$. <p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Rank_(linear_algebra)\" style=\"color: #cc0000\">Rank of a Matrix</a></p>\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Transpose\" style=\"color: #cc0000\">Transpose</a></p> <p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Expected_value\" style=\"color: #cc0000\">Expected Value</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "💻 ```model.matrix``` creates a design matrix based on the declared predictors and includes a vector of ones by default as the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## asking R to print the design matrix for the chosen model\n",
    "%R X <- model.matrix(f,data=hprice2)\n",
    "%R dim(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## asking Python to print the design matrix for the chose model\n",
    "import patsy\n",
    "y, X = patsy.dmatrices(f,hprice2,return_type='matrix')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## calculating & printing the sample counterpart of E[xx']\n",
    "%R X.X.n <- t(X)%*%X/nrow(X)\n",
    "%R X.X.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating & printing the sample counterpart of E[xx']\n",
    "import numpy as np\n",
    "X_X_n = X.transpose()@X/X.shape[0]\n",
    "print(X_X_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## asking R to calculate the actual rank of the estimated E[xx']\n",
    "%R qr(X.X.n)$rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## asking Python to calculate the actual rank of the estimated E[xx']\n",
    "print(np.linalg.matrix_rank(X_X_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<span style=\"color:blue\">Assumption MLR.4:</span>** $E[\\mathbf{e}|\\mathbf{X}]=\\mathbf{0}$.\n",
    "\n",
    "$$\n",
    "E[\\mathbf{e}|\\mathbf{X}]=\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "E[e_{1}|\\mathbf{X}]\\\\\n",
    "E[e_{2}|\\mathbf{X}]\\\\\n",
    "E[e_{3}|\\mathbf{X}]\\\\\n",
    "\\vdots\\\\\n",
    "E[e_{n}|\\mathbf{X}]\n",
    "\\end{array}\n",
    "\\right]  =\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]=\\mathbf{0}.  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<span style=\"color:blue\">Assumption MLR.5:</span>** var$(\\mathbf{e}|\\mathbf{X})=E[\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X}]=\\mathbf{D}$, i.e.,\n",
    "\n",
    "$$\n",
    "E[\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X}]=\n",
    "\\begin{bmatrix}\n",
    "E[e_{1}^{2}|\\mathbf{X}] & 0 & \\cdots & 0\\\\\n",
    "0 & E[e_{2}^{2}|\\mathbf{X}] & \\cdots & 0\\\\\n",
    "0 & 0 & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\cdots & E[e_{n}^{2}|\\mathbf{X}]\n",
    "\\end{bmatrix}  = \\mathbf{D}\n",
    "$$\n",
    "\n",
    "where $\\vert\\mathbf{D}\\vert < +\\infty$.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Determinant\" style=\"color: #cc0000\">Determinant</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The *Ordinary Least Squares* (OLS) Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Define the Sum of Squared Errors ($\\mathrm{SSE}$) as a function of any candidate guess, $\\mathbf{b}=[b_{0},b_{1},\\cdots,b_{k}]^{\\prime}$, for the unknown $[\\beta_{0},\\beta_{1},\\beta_{2},\\cdots,\\beta_{k}]^{\\prime}\\equiv\\mathbf{\\beta}$, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathrm{SSE}(\\mathbf{b})=\\Sigma_{i=1}^{n}(y_{i}-b_{0}-b_{1}x_{1,i}-b_{2}x_{2,i}\n",
    "-\\cdots-b_{k}x_{k,i})^{2}=(\\mathbf{y}-\\mathbf{Xb})^{\\prime}(\\mathbf{y}\n",
    "-\\mathbf{Xb})\n",
    "$$\n",
    "\n",
    "By standard [matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus) one has\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{SSE}(\\mathbf{b})}{\\partial\\mathbf{b}}=\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{0}\\\\\n",
    "\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{1}\\\\\n",
    "\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\\\\n",
    "\\vdots\\\\\n",
    "\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}\n",
    "\\end{array}\n",
    "\\right]  =-2\\mathbf{X}^{\\prime}(\\mathbf{y}-\\mathbf{Xb})\\text{,}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^{2}\\mathrm{SSE}(\\mathbf{b})}{\\partial\\mathbf{b}\\partial\\mathbf{b}\n",
    "^{\\prime}}=\\left[\n",
    "\\begin{array}\n",
    "[c]{cccc}\n",
    "\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{0}^{2} & \\partial^{2}\\mathrm{SSE}(\\mathbf{b}\n",
    ")/\\partial b_{0}\\partial b_{1} & \\cdots & \\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial\n",
    "b_{0}\\partial b_{k}\\\\\n",
    "\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{1}\\partial b_{0} & \\partial\n",
    "^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{1}^{2} & \\cdots & \\partial^{2}\\mathrm{SSE}(\\mathbf{b}\n",
    ")/\\partial b_{1}\\partial b_{k}\\\\\n",
    "\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\partial b_{0} & \\partial\n",
    "^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\partial b_{1} & \\cdots & \\partial\n",
    "^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\partial b_{k}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}\\partial b_{0} & \\partial\n",
    "^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}\\partial b_{1} & \\cdots & \\partial\n",
    "^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}^{2}\n",
    "\\end{array}\n",
    "\\right]  =2\\mathbf{X}^{\\prime}\\mathbf{X}\\text{,}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}^{\\prime}\\mathbf{X}$ is a [positive definite matrix](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix) by **<span style=\"color:blue\">Assumption MLR.3</span>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## printing 2X'X\n",
    "%R 2*t(X)%*%X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## printing 2X'X\n",
    "print(2*X.transpose()@X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Therefore $\\left.  \\partial \\mathrm{SSE}(\\mathbf{b})/\\partial\\mathbf{b}\\right\\vert _{\\mathbf{b}=\\widehat{\\boldsymbol{\\beta}}\n",
    "}=\\mathbf{0}$ defines a minima, i.e.,\n",
    "\n",
    "$$\\widehat{\\boldsymbol{\\beta}}=(\\mathbf{X}\n",
    "^{\\prime}\\mathbf{X})^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}.$$\n",
    "\n",
    "Here we have used the\n",
    "notation $\\mathbf{A}^{-1}$ to denote the inverse of a matrix $\\mathbf{A}$.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\" style=\"color: #cc0000\">Euclidean Distance</a></p>\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Invertible_matrix\" style=\"color: #cc0000\">Inverse</a></p>\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Maxima_and_minima\" style=\"color: #cc0000\">Maxima</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "💻 Calculating the OLS estimator by hand first and then using the highly optimized ```lm``` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## calculating OLS by hand\n",
    "%R solve(t(X)%*%X)%*%(t(X)%*%hprice2$lprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating OLS by hand\n",
    "from numpy.linalg import inv\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(inv(X.transpose()@X)@X.transpose()@y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## calculating OLS using the `lm' command in R\n",
    "%R coef(lm(f,data=hprice2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating OLS using the `formula' framework in statsmodels in Python\n",
    "import statsmodels.formula.api as smf\n",
    "ols = smf.ols(formula=f,data=hprice2).fit()\n",
    "print(ols.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The _Algebra_ of the OLS Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "💻 We now save the ```lm``` object and call it ```ols```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%R ols <- lm(f,data=hprice2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = smf.ols(formula=f,data=hprice2).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Fitted Values*: $\\widehat{\\mathbf{y}}=\\mathbf{X}\\widehat{\\boldsymbol{\\beta}}$.\n",
    "\n",
    "*Residuals*: $\\widehat{\\mathbf{e}}=\\mathbf{y}-\\mathbf{X} \\widehat{\\boldsymbol{\\beta}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## OLS: printing the first 6 outcomes, fitted values, & residuals\n",
    "%R head(data.frame(y=hprice2$lprice,y.hat=fitted(ols),e.hat=resid(ols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: printing the first 6 outcomes, fitted values, & residuals\n",
    "import pandas as pd\n",
    "pd.DataFrame({'y':hprice2['lprice'],'y_hat':ols.fittedvalues.values,'e_hat':ols.resid}).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Orthogonality*: $\\mathbf{X}^{\\prime}\\widehat{\\mathbf{e}}=\\mathbf{0}$.\n",
    "\n",
    "✏️ This follows from $\\left.  \\partial \\mathrm{SSE}(\\mathbf{b})/\\partial\\mathbf{b}\\right\\vert _{\\mathbf{b}=\\widehat{\\boldsymbol{\\beta}}\n",
    "}=\\mathbf{0}$, since $$-2\\mathbf{X}^{\\prime}(\\mathbf{y}-\\mathbf{X\\widehat{\\boldsymbol{\\beta}}})=-2\\mathbf{X}^{\\prime}\\widehat{\\mathbf{e}}=\\mathbf{0}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## OLS: showing the orthogonality of the residuals and predictors\n",
    "%R round(t(X)%*%resid(ols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: showing the orthogonality of the residuals and predictors\n",
    "np.round(X.transpose()@ols.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Analysis-of-variance*:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}&=\\sum_{i=1}^{n}\\left(\\widehat{y}_{i}-\\bar{y}\\right)^{2}+\\sum_{i=1}^{n} \\widehat{e}_{i}^{2},\\\\\n",
    "\\text{Total Sum of Squares}&=\\text{Explained Sum of Squares} + \\text{Residual Sum of Squares},\\\\\n",
    "\\left(\\mathbf{y}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)^{\\prime}\\left(\\mathbf{y}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)&=\\left(\\widehat{\\mathbf{y}}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)^{\\prime}\\left(\\widehat{\\mathbf{y}}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)+\\widehat{\\mathbf{e}}^{\\prime} \\widehat{\\mathbf{e}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\iota}_n$ is a $n\\times 1$ vector of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## OLS: total sum of squares\n",
    "%R print(sum(anova(ols)[,2]))\n",
    "\n",
    "## OLS: explained sum of squares\n",
    "%R print(sum(anova(ols)[-9,2]))\n",
    "\n",
    "## OLS: residual sum of squares\n",
    "%R print(sum(anova(ols)[9,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: total sum of squares\n",
    "print(ols.centered_tss)\n",
    "\n",
    "## OLS: explained sum of squares\n",
    "print(ols.ess)\n",
    "\n",
    "## OLS: residual sum of squares\n",
    "print(ols.ssr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Coefficient of Determination* (*$R^2$*):\n",
    "\n",
    "$$\n",
    "R^{2}=\\frac{\\sum_{i=1}^{n}\\left(\\widehat{y}_{i}-\\bar{y}\\right)^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}=1-\\frac{\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}.\n",
    "$$\n",
    "\n",
    "✏️ More generally the $R^2$ in most models (linear and *non-linear*) is defined as $(\\widehat{\\text{corr}(\\widehat{y},y)})^2$, i.e., the squared of the sample correlation coefficient between the true values and the fitted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## OLS: R2 (manually)\n",
    "%R print(sum(anova(ols)[-9,2])/sum(anova(ols)[,2]))\n",
    "\n",
    "## OLS: R2 (from lm)\n",
    "%R print(summary(ols)$r.squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: R2 (manually)\n",
    "print(ols.ess/ols.centered_tss)\n",
    "\n",
    "## OLS: R2 (from lm)\n",
    "ols.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Adjusted R-squared* ($\\overline{R}^2$):\n",
    "\n",
    "$$\n",
    "\\overline{R}^{2}=1-\\frac{(n-1) \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{(n-k-1) \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "✏️ Unlike the $R^2$ which cannot decrease as $k$ increases, $\\overline{R}^2$ can either increase _or_ decrease with $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## OLS: adj. R2 (manually)\n",
    "%R print(1-(sum(anova(ols)[9,2])/sum(anova(ols)[,2]))*(sum(anova(ols)[,1])/anova(ols)[9,1]))\n",
    "\n",
    "## OLS: adj. R2 (from lm)\n",
    "%R print(summary(ols)$adj.r.squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: adj. R2 (manually)\n",
    "print(1-((ols.nobs-1)/ols.df_resid)*(ols.ssr/ols.centered_tss))\n",
    "\n",
    "## OLS: adj. R2 (from lm)\n",
    "ols.rsquared_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Leverage Values*:\n",
    "\n",
    "The [leverage values](https://en.wikipedia.org/wiki/Leverage_(statistics)) for the design matrix $\\mathbf{X}$ are the [diagonal](https://en.wikipedia.org/wiki/Main_diagonal) elements of the matrix $\\mathbf{X}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime}$. There are $n$ leverage values, and are typically written as $h_{i i}$ for $i=1, \\ldots, n$. since\n",
    "\n",
    "$$\n",
    "\\mathbf{X}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime}=\n",
    "\\left(\\begin{array}{c}{\\mathbf{x}_{1}^{\\prime}} \\\\ {\\mathbf{x}_{2}^{\\prime}} \\\\ {\\vdots} \\\\ {\\mathbf{x}_{n}^{\\prime}}\\end{array}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\begin{array}{llll}{\\mathbf{x}_{1}} & {\\mathbf{x}_{2}} & {\\cdots} & {\\mathbf{x}_{n}}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "they are\n",
    "$$\n",
    "h_{i i}=\\mathbf{x}_{i}^{\\prime}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{x}_{i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## OLS: leverage values\n",
    "%R hii <- hatvalues(ols)\n",
    "%R head(hii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: leverage values\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "\n",
    "hii = OLSInfluence(ols).hat_matrix_diag\n",
    "hii[0:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Prediction Error* (leave-one-out residual or prediction residual):\n",
    "\n",
    "$$\\widetilde{e}_{i}=y_{i}-\\widetilde{y}_{i},$$ \n",
    "\n",
    "where we use the leave-one-out predicted value for $y_i$, i.e.,\n",
    "\n",
    "$$\n",
    "\\widetilde{y}_{i}=\\mathbf{x}_{i}^{\\prime} \\widehat{\\boldsymbol{\\beta}}_{(-i)},$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\widehat{\\boldsymbol{\\beta}}_{(-i)} &=\\left(\\sum_{j \\neq i} \\mathbf{x}_{j} \\mathbf{x}_{j}^{\\prime}\\right)^{-1}\\left(\\sum_{j \\neq i} \\mathbf{x}_{j} y_{j}\\right) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}-\\mathbf{x}_{i} \\mathbf{x}_{i}^{\\prime}\\right)^{-1}\\left(\\mathbf{X}^{\\prime} \\mathbf{y}-\\mathbf{x}_{i} y_{i}\\right) \\\\ &=\\left(\\mathbf{X}_{(-i)}^{\\prime} \\mathbf{X}_{(-i)}\\right)^{-1} \\mathbf{X}_{(-i)}^{\\prime} \\mathbf{y}_{(-i)}. \\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{X}_{(-i)}$ and $\\mathbf{y}_{(-i)}$ are the data matrices omitting the $i$th row. The notation $\\widehat{\\boldsymbol{\\beta}}_{(-i)}$ or $\\widehat{\\boldsymbol{\\beta}}_{-i}$ is commonly\n",
    "used to denote an estimator with the $i$th observation omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "✏️ There is a leave-one-out estimator for each observation, $i=1,\\ldots,n$ so we have $n$ such estimators, and one can show that\n",
    "\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}_{(-i)}=\\widehat{\\boldsymbol{\\beta}}-\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{x}_{i} \\widetilde{e}_{i},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\widetilde{e}_{i}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## OLS: original residuals\n",
    "%R e.hat <- resid(ols)\n",
    "\n",
    "## OLS: calculating the prediction error\n",
    "%R e.tilde <- resid(ols)/(1-hii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: original residuals\n",
    "e_hat = ols.resid\n",
    "\n",
    "## OLS: calculating the prediction error\n",
    "e_tilde = ols.resid/(1-hii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## OLS: manually re-calculating OLS without observation 156\n",
    "%R data.frame(beta.hat=coef(ols),beta.hat.i=coef(ols)-solve(t(X)%*%X)%*%X[156,]%*%e.tilde[156])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: manually re-calculating OLS without observation 156\n",
    "pd.DataFrame({'beta_hat':ols.params,\n",
    "           'beta_hat_i':ols.params-inv(X.transpose()@X)@(X[155,]*e_tilde[155])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Estimation of Error Variance*:\n",
    "\n",
    "The _unconditional_ error variance $\\sigma^2=E(e^2_{i})$ can be estimated as\n",
    "\n",
    "1. Estimator 1:\n",
    "\n",
    "$$s^{2}=\\frac{1}{n-k-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}.$$\n",
    "\n",
    "2. Estimator 2:\n",
    "\n",
    "$$\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}.$$\n",
    "\n",
    "3. Estimator 3:\n",
    "\n",
    "$$\\bar{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\tilde{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} \\widehat{e}_{i}^{2}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "✏️ When $k / n$ is small (typically, this occurs when $n$ is large), the estimators $\\widehat{\\sigma}^{2}, s^{2}$ and $\\overline{\\sigma}^{2}$ are likely to be similar to one another. However, if $k / n$ is large then $s^{2}$ and $\\overline{\\sigma}^{2}$ are generally preferred to $\\widehat{\\sigma}^{2}$. Consequently it is best to use one of the bias-corrected variance estimators in applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## OLS: error variance estimators\n",
    "%R s2 <- sum(e.hat^2)/(length(e.hat)-dim(X)[2])\n",
    "%R sigma.hat2 <- sum(e.hat^2)/length(e.hat)\n",
    "%R sigma.bar2 <- sum((e.hat/(1-hii))^2)/length(e.hat)\n",
    "%R data.frame(s2=s2,sigma.hat2=sigma.hat2,sigma.bar2=sigma.bar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: error variance estimators\n",
    "s2 = ols.ssr/ols.df_resid\n",
    "sigma_hat2 = ols.ssr/ols.nobs\n",
    "sigma_bar2 = ((e_hat/(1-hii))**2).sum(axis=0)/ols.nobs\n",
    "\n",
    "## Because all values are 'scalars' we need to pass them inside []\n",
    "print(pd.DataFrame({'s2':[s2],'sigma_hat2':[sigma_hat2],'sigma_bar2':[sigma_bar2]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The _Finite Sample_ Properties of the OLS Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<span style=\"color:blue\">Mean of OLS:</span>** Under Assumptions MLR.1, MLR.2, MLR.3, and MLR.4 one has\n",
    "\n",
    "$\\begin{aligned} E(\\widehat{\\boldsymbol{\\beta}} | \\mathbf{X}) &=E\\left(\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} | \\mathbf{X}\\right) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} E(\\mathbf{y} | \\mathbf{X}) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{X} \\boldsymbol{\\beta} \\\\ &=\\boldsymbol{\\beta} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<span style=\"color:blue\">Variance of OLS:</span>** Firstly, let us use the notation $\\mathbf{V}_{\\widehat{\\beta}} \\stackrel{d e f}{=} \\text{var}(\\widehat{\\boldsymbol{\\beta}} | \\mathbf{X})$ and recall from Assumption MLR.5 that var$(\\mathbf{e}|\\mathbf{X})=E[\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X}]=\\mathbf{D}$. Then under Assumptions MLR.1, MLR.2, MLR.3, MLR.4, and MLR.5 one has\n",
    "\n",
    "$\\begin{aligned} \\mathbf{V}_{\\widehat{\\beta}} &=\\text{var}(\\widehat{\\boldsymbol{\\beta}} | \\mathbf{X}) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}^{\\prime} \\mathbf{D} \\mathbf{X}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "✏️ If one has *homoskedasticity*, i.e., $\\mathbf{D}=\\sigma^2\\mathbf{I}_n$, then $\\mathbf{V}_{\\widehat{\\boldsymbol{\\beta}}}=\\sigma^{2}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Asymptotic Properties of the OLS Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<span style=\"color:blue\">(Asymptotic) Consistency of OLS:</span>** As $n\\rightarrow\\infty$, under Assumptions MLR.1, MLR.2, MLR.3 and MLR.4 one has\n",
    "\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}\\stackrel{p}{\\longrightarrow} \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_probability\" style=\"color: #cc0000\">Convergence in probability</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<span style=\"color:blue\">(Asymptotic) Distribution of OLS:</span>** As $n\\rightarrow\\infty$, under Assumptions MLR.1, MLR.2, MLR.3, MLR.4, and MLR.5 one has\n",
    "\n",
    "$$\\sqrt{n}(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}) \\stackrel{d}{\\longrightarrow} \\mathbf{N}\\left(\\mathbf{0}, \\mathbf{V}_{\\boldsymbol{\\beta}}\\right),$$\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_distribution\" style=\"color: #cc0000\">Convergence in Distribution</a></p>\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_{\\boldsymbol{\\beta}}=E(\\mathbf{x}_i\\mathbf{x}_i^{\\prime})^{-1}E[\\mathbf{x}_{i}E(e_i^2|\\mathbf{x}_i)\\mathbf{x}_{i}^{\\prime}]E(\\mathbf{x}_i\\mathbf{x}_i^{\\prime})^{-1},\n",
    "$$\n",
    "\n",
    "and the notation $\\mathbf{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ denotes a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). $\\mathbf{V}_{\\boldsymbol{\\beta}}$ is the asymptotic (asy.) [variance-covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix) of $\\sqrt{n}(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})$ and therefore $n^{-1}\\mathbf{V}_{\\boldsymbol{\\beta}}$ is the asymptotic variance of $\\widehat{\\boldsymbol{\\beta}}$ and it is generally _unknown_ and needs to be estimated.\n",
    "\n",
    "$$\n",
    "n^{-1}\\mathbf{V}_{\\boldsymbol{\\beta}}=\\left[\n",
    "\\begin{array}\n",
    "[c]{cccc}\n",
    "\\text{asy. var}(\\widehat{\\beta}_{0}) & \\text{asy. cov}(\\widehat{\\beta}\n",
    "_{0},\\widehat{\\beta}_{1}) & \\cdots & \\text{asy. cov}(\\widehat{\\beta}\n",
    "_{0},\\widehat{\\beta}_{k})\\\\\n",
    "\\text{asy. cov}(\\widehat{\\beta}_{1},\\widehat{\\beta}_{0}) & \\text{asy.\n",
    "var}(\\widehat{\\beta}_{1}) & \\cdots & \\text{asy. cov}(\\widehat{\\beta}\n",
    "_{1},\\widehat{\\beta}_{k})\\\\\n",
    "\\text{asy. cov}(\\widehat{\\beta}_{2},\\widehat{\\beta}_{0}) & \\text{asy.\n",
    "cov}(\\widehat{\\beta}_{2},\\widehat{\\beta}_{1}) & \\cdots & \\text{asy.\n",
    "cov}(\\widehat{\\beta}_{2},\\widehat{\\beta}_{k})\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\text{asy. cov}(\\widehat{\\beta}_{k},\\widehat{\\beta}_{0}) & \\text{asy.\n",
    "cov}(\\widehat{\\beta}_{k},\\widehat{\\beta}_{1}) & \\cdots & \\text{asy.\n",
    "var}(\\widehat{\\beta}_{k})\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "✏️ Notice that $\\mathbf{V}_{\\boldsymbol{\\beta}}$ is a $(k+1)\\times(k+1)$ [square](https://en.wikipedia.org/wiki/Square_matrix), [symmetric](https://en.wikipedia.org/wiki/Symmetric_matrix), and [positive semi-definite](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix) matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<ins>(Asymptotic) OLS Variance Estimation</ins>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Firstly notice that\n",
    "\n",
    "$$\n",
    "\\begin{aligned} n\\mathbf{V}_{\\widehat{\\boldsymbol{\\beta}}} &=n\\text{ var}(\\widehat{\\boldsymbol{\\beta}} | \\mathbf{X}) \\\\ &=\\left(n^{-1}\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(n^{-1}\\mathbf{X}^{\\prime} \\mathbf{D} \\mathbf{X}\\right)\\left(n^{-1}\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}, \\end{aligned}\n",
    "$$\n",
    "\n",
    "and it has been shown that\n",
    "\n",
    "$$n \\mathbf{V}_{\\widehat{\\boldsymbol{\\beta}}} \\stackrel{p}{\\longrightarrow} \\mathbf{V}_{\\boldsymbol{\\beta}}$$\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_probability\" style=\"color: #cc0000\">Convergence in probability</a></p>\n",
    "\n",
    "🛑 Unfortunately $n\\mathbf{V}_{\\widehat{\\boldsymbol{\\beta}}}$ is _infeasible_ as matrix $\\mathbf{D}$ (defined in Assumption MLR.5) is *not known*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<span style=\"color:red\">HC0:</span>**\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{V}}_{\\widehat{\\boldsymbol{\\beta}}}^{\\mathrm{HC0}}=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\mathbf{x}_{i} \\widehat{e}_{i}^{2}\\mathbf{x}_{i}^{\\prime} \\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "**<span style=\"color:red\">HC1:</span>** (most common in *econometrics*)\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{V}}_{\\widehat{\\boldsymbol{\\beta}}}^{\\mathrm{HCl}}=\\left(\\frac{n}{n-k-1}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\mathbf{x}_{i} \\widehat{e}_{i}^{2}\\mathbf{x}_{i}^{\\prime}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "**<span style=\"color:red\">HC2:</span>**\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{V}}_{\\widehat{\\boldsymbol{\\beta}}}^{\\mathrm{HC2}}=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\mathbf{x}_{i} \\left(1-h_{i i}\\right)^{-1}\\widehat{e}_{i}^{2} \\mathbf{x}_{i}^{\\prime} \\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "**<span style=\"color:red\">HC3:</span>**\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{V}}_{\\widehat{\\boldsymbol{\\beta}}}^{\\mathrm{HC3}}=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\mathbf{x}_{i} \\left(1-h_{i i}\\right)^{-2}\\widehat{e}_{i}^{2} \\mathbf{x}_{i}^{\\prime} \\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 Unfortunately the default variance covariance matrix reported in ```lm``` corresponds to the _homoskedastic_ case, i.e., $\\widehat{\\sigma}^2(\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}$. All four estimators can be computed using the ```sandwich``` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'sandwich' package if not previously installed\n",
    "%R if (!require(sandwich)) install.packages('sandwich')\n",
    "\n",
    "## loading the packages\n",
    "%R library(sandwich)\n",
    "\n",
    "## OLS: calculating estimated asymptotic vcov matrices\n",
    "#%R vcovHC(ols,type=\"HC0\")\n",
    "%R vcovHC(ols,type=\"HC1\")\n",
    "#%R vcovHC(ols,type=\"HC2\")\n",
    "#%R vcovHC(ols,type=\"HC3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: calculating estimated asymptotic vcov matrices\n",
    "#print(ols.cov_HC0)\n",
    "print(ols.cov_HC1)\n",
    "#print(ols.cov_HC2)\n",
    "#print(ols.cov_HC3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<ins>(Asymptotic) Standard Errors</ins>:\n",
    "\n",
    "These corresponds to the estimator of the standard deviation of the distribution of the OLS estimator $\\widehat{\\boldsymbol{\\beta}}$, i.e., \n",
    "\n",
    "$$\n",
    "s\\left(\\widehat{\\beta}_{j}\\right)=\\sqrt{\\widehat{\\mathbf{V}}^{\\mathrm{HC?}}_{\\widehat{\\beta}_{j}}}=\\sqrt{\\left[\\widehat{\\mathbf{V}}^{\\mathrm{HC?}}_{\\widehat{\\boldsymbol{\\beta}}}\\right]_{j j}},\n",
    "$$\n",
    "\n",
    "for $?=0,1,2,3$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "💻 Computationally they simply correspond to the square root of the main diagonal elements of the $\\widehat{\\mathbf{V}}^{\\mathrm{HC0}}_{\\widehat{\\boldsymbol{\\beta}}}$, $\\widehat{\\mathbf{V}}^{\\mathrm{HC1}}_{\\widehat{\\boldsymbol{\\beta}}}$, $\\widehat{\\mathbf{V}}^{\\mathrm{HC2}}_{\\widehat{\\boldsymbol{\\beta}}}$, or $\\widehat{\\mathbf{V}}^{\\mathrm{HC3}}_{\\widehat{\\boldsymbol{\\beta}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%R data.frame(se.HC0=sqrt(diag(vcovHC(ols,type=\"HC0\"))),se.HC1=sqrt(diag(vcovHC(ols,type=\"HC1\"))),se.HC2=sqrt(diag(vcovHC(ols,type=\"HC2\"))),se.HC3=sqrt(diag(vcovHC(ols,type=\"HC3\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'se_HC0':ols.HC0_se,'se_HC1':ols.HC1_se,'se_HC2':ols.HC2_se,'se_HC3':ols.HC3_se})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<ins>$t$-statistic</ins>:\n",
    "\n",
    "$$t=\\frac{\\widehat{\\beta}_j-\\beta_j}{s(\\widehat{\\beta}_j)}\\stackrel{d}{\\longrightarrow}N(0,1).$$\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/T-statistic\" style=\"color: #cc0000\">t-statistic</a></p>\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Normal_distribution#Definition\" style=\"color: #cc0000\">Standard Normal</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "💻 Most statistical packages will use the Student's $t$ distribution with $n-k-1$ degrees-of-freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'lmtest' package if not previously installed\n",
    "%R if (!require(lmtest)) install.packages('lmtest')\n",
    "\n",
    "## loading the packages\n",
    "%R library(lmtest)\n",
    "\n",
    "## OLS: calculating standard t-statistics for 'significance'\n",
    "%R coeftest(ols, vcov = vcovHC, type = 'HC1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smf.ols(formula=f,data=hprice2).fit(cov_type='HC1').summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<ins>(Asymptotic) Confidence Interval</ins>:\n",
    "\n",
    "The OLS estimator $\\widehat{\\beta}_j$ is called a _point estimator_ of the unknown slope parameter $\\beta_j$. A broader concept is a _set estimator_ $\\widehat{C}=[\\widehat{L},\\widehat{U}]$ which is an _interval estimator_ of $\\beta_j$. An interval estimator $\\widehat{C}$ is called a **confidence interval** when the goal is to set the coverage probability\n",
    "to equal a pre-specified target such as $90 \\%$ or $95 \\%$. The conventional confidence interval for $\\beta_j$ takes the form\n",
    "\n",
    "$$\n",
    "\\widehat{C}=[\\widehat{\\beta}_j-c\\cdot s(\\widehat{\\beta}_j),\\widehat{\\beta}_j+c\\cdot s(\\widehat{\\beta}_j)],\n",
    "$$\n",
    "\n",
    "where $c$ equals the $1-\\alpha$ [quantile](https://en.wikipedia.org/wiki/Quantile) of the distribution of $|Z|$, i.e., $c$ is equivalent to the $1-\\alpha/2$ quantile of the standard normal distribution.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Confidence_interval\" style=\"color: #cc0000\">Confidence Interval</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## OLS: calculating asymptotic 90% confidence interval\n",
    "%R coefci(ols, level = 0.90, vcov = vcovHC, type = 'HC1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ The `statsmodels` package in Python uses the quantiles from a *_t_*-distribution instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smf.ols(formula=f,data=hprice2).fit(cov_type='HC1').conf_int(alpha=0.10, cols=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the linear regression model the conditional mean of $y_{i}$ given $\\mathbf{x}_{i}=\\mathbf{x}$ is\n",
    "$$\n",
    "m(\\mathbf{x})=E\\left(y_{i} | \\mathbf{x}_{i}=\\mathbf{x}\\right)=\\mathbf{x}^{\\prime} \\beta\n",
    "$$\n",
    "Thus an asymptotic 95% confidence interval for $m(\\mathbf{x})$ is\n",
    "$$\n",
    "\\left[\\mathbf{x}^{\\prime} \\widehat{\\boldsymbol{\\beta}} \\pm 1.96 \\sqrt{\\mathbf{x}^{\\prime} \\widehat{\\mathbf{V}}_{\\widehat{\\boldsymbol{\\beta}}}^{\\mathrm{HC?}} \\mathbf{x}}\\right]\n",
    "$$\n",
    "for $?=0,1,2,3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'mgcv' package if not previously installed\n",
    "%R if (!require(mgcv)) install.packages('mgcv')\n",
    "\n",
    "## loading the packages\n",
    "%R library(mgcv)\n",
    "\n",
    "## OLS: performing OLS using the 'gam' function in 'mgcv'\n",
    "%R ols.gam <- gam(f,data=hprice2)\n",
    "\n",
    "## OLS: replacing the default homoskedastic vcov\n",
    "%R ols.gam$Vp <- vcovHC(ols,type='HC1')\n",
    "\n",
    "## OLS: creating the x_i=mean(x)\n",
    "%R attach(hprice2)\n",
    "%R new.dat=data.frame(lnox=mean(lnox),lproptax=mean(lproptax),crime=mean(crime),rooms=mean(rooms),dist=mean(dist),radial=mean(radial),stratio=mean(stratio),lowstat=mean(lowstat))\n",
    "%R detach(hprice2)\n",
    "\n",
    "## OLS: using the predict command to get the robust s.e.\n",
    "%R reg.int <- predict(ols.gam,newdata=new.dat,se.fit=TRUE)\n",
    "\n",
    "## OLS: manually estimating the (robust) regression interval\n",
    "%R data.frame(L.hat=reg.int$fit-1.96*reg.int$se.fit,U.hat=reg.int$fit+1.96*reg.int$se.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "## OLS: creating the x_i=mean(x)\n",
    "r_ols = sm.OLS(y,X).fit(cov_type='HC1')\n",
    "\n",
    "## OLS: creating the x_i=mean(x)\n",
    "pred_mean_x = r_ols.get_prediction(X.mean(axis=0))\n",
    "pd.DataFrame({'L.hat':pred_mean_x.summary_frame(alpha=0.05)['mean_ci_lower'],\n",
    "              'U.hat':pred_mean_x.summary_frame(alpha=0.05)['mean_ci_upper']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forecast Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we are given a value of the regressor vector $\\mathbf{x}_{n+1}$ for an individual outside the sample, and\n",
    "we want to forecast (guess) $y_{n+1}$ for this individual. This is equivalent to forecasting $y_{n+1}$ given $\\mathbf{x}_{n+1}=\\mathbf{x}$, which will generally be a function of $\\mathbf{x}$. A reasonable forecasting rule is the conditional mean $m(\\mathbf{x})$ as it is the mean-square-minimizing forecast. A point forecast is the estimated conditional mean $\\widehat{m}(\\mathbf{x})=\\mathbf{x}^{\\prime} \\widehat{\\beta}$. We would also like a measure of uncertainty for the forecast.\n",
    "\n",
    "The forecast error is $\\widehat{e}_{n+1}=y_{n+1}-\\widehat{m}(\\mathbf{x})=e_{n+1}-\\mathbf{x}^{\\prime}(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})$. As the out-of-sample error $e_{n+1}$ is independent of the in-sample estimate $\\widehat{\\boldsymbol{\\beta}}$, this has conditional variance\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E\\left(\\widehat{e}_{n+1}^{2} | \\mathbf{x}_{n+1}=\\mathbf{x}\\right) &=E\\left(e_{n+1}^{2}-2 \\mathbf{x}^{\\prime}(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}) e_{n+1}+\\mathbf{x}^{\\prime}(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})^{\\prime} \\mathbf{x} | \\mathbf{x}_{n+1}=\\mathbf{x}\\right) \\\\ &=E\\left(e_{n+1}^{2} | \\mathbf{x}_{n+1}=\\mathbf{x}\\right)+\\mathbf{x}^{\\prime} E[(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})^{\\prime}|\\mathbf{x}_{n+1}=\\mathbf{x}] \\mathbf{x} \\\\ &=\\sigma^{2}(\\mathbf{x})+\\mathbf{x}^{\\prime} \\mathbf{V}_{\\widehat{\\boldsymbol{\\beta}}} \\mathbf{x} \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Under homoskedasticity $E\\left(e_{n+1}^{2} | \\mathbf{x}_{n+1}\\right)=\\sigma^{2}$. In this case $\\widehat{\\sigma}^{2}+\\mathbf{x}^{\\prime} \\widehat{\\mathbf{V}}_{\\widehat{\\beta}}^{\\mathrm{HC?}} \\mathbf{x}$ is a consistent estimator of this conditional variance, so a standard error for the forecast is $\\widehat{s}(\\mathbf{x})=\\sqrt{\\widehat{\\sigma}^{2}+\\mathbf{x}^{\\prime} \\widehat{\\mathbf{V}}_{\\widehat{\\boldsymbol{\\beta}}}^{\\mathrm{HC?}} \\mathbf{x}}$, and the conventional 95% forecast insterval for $y_{n+1}$ uses a normal approximation and sets\n",
    "\n",
    "$$\\left[\\mathbf{x}^{\\prime} \\widehat{\\boldsymbol{\\beta}} \\pm 1.96 \\widehat{s}(\\mathbf{x})\\right].$$\n",
    "\n",
    "🛑 This interval is only valid if $e_{n+1}$ comes from a _homoskedastic_ normal distribution, i.e., $e_{n+1}\\sim N(0,\\sigma^2)$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 We now _manually_ calculate the forecast interval for the same mean values of $\\mathbf{x}$ as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## OLS: manually estimating the forecast interval\n",
    "%R s.hat <- sqrt(summary(ols)$sigma^2+reg.int$se.fit^2)\n",
    "%R data.frame(L.hat=reg.int$fit-1.96*s.hat,U.hat=reg.int$fit+1.96*s.hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: creating the x_i=mean(x)\n",
    "pred_mean_x = r_ols.get_prediction(X.mean(axis=0))\n",
    "pd.DataFrame({'L.hat':pred_mean_x.summary_frame(alpha=0.05)['obs_ci_lower'],\n",
    "              'U.hat':pred_mean_x.summary_frame(alpha=0.05)['obs_ci_upper']})\n"
   ]
  }
 ],
 "metadata": {
  "author": "lde",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
